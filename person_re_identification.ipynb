{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5433daed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "122b0b2f",
   "metadata": {},
   "source": [
    "### Person Re_Idenfication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62785c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output, display, Image\n",
    "from ultralytics import YOLO\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "names = model.model.names\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.model = models.resnet50(pretrained=True).eval()\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    \n",
    "    def extract_features(self, image, bbox):\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        cropped_image = image[y1:y2, x1:x2]\n",
    "        if cropped_image.size == 0:\n",
    "            return torch.zeros((1, 2048)) \n",
    "        input_tensor = self.transform(cropped_image).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            features = self.model(input_tensor).flatten()\n",
    "        return features\n",
    "\n",
    "feature_extractor = FeatureExtractor()\n",
    "track_history = defaultdict(lambda: [])\n",
    "track_features = {}\n",
    "next_track_id = 0\n",
    "track_info = []\n",
    "\n",
    "class KalmanFilter:\n",
    "    def __init__(self, dt, u_x, u_y, std_acc, x_std_meas, y_std_meas):\n",
    "        self.dt = dt\n",
    "        self.u = np.matrix([[u_x], [u_y]])\n",
    "        self.x = np.matrix([[0], [0], [0], [0]])\n",
    "        self.A = np.matrix([[1, 0, self.dt, 0], [0, 1, 0, self.dt], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
    "        self.B = np.matrix([[(self.dt**2) / 2, 0], [0, (self.dt**2) / 2], [self.dt, 0], [0, self.dt]])\n",
    "        self.H = np.matrix([[1, 0, 0, 0], [0, 1, 0, 0]])\n",
    "        self.Q = np.matrix([[self.dt**4 / 4, 0, self.dt**3 / 2, 0],\n",
    "                            [0, self.dt**4 / 4, 0, self.dt**3 / 2],\n",
    "                            [self.dt**3 / 2, 0, self.dt**2, 0],\n",
    "                            [0, self.dt**3 / 2, 0, self.dt**2]]) * std_acc**2\n",
    "        self.R = np.matrix([[x_std_meas**2, 0], [0, y_std_meas**2]])\n",
    "        self.P = np.eye(self.A.shape[1])\n",
    "\n",
    "    def predict(self):\n",
    "        self.x = np.dot(self.A, self.x) + np.dot(self.B, self.u)\n",
    "        self.P = np.dot(np.dot(self.A, self.P), self.A.T) + self.Q\n",
    "        return self.x[0:2]\n",
    "\n",
    "    def update(self, z):\n",
    "        S = np.dot(self.H, np.dot(self.P, self.H.T)) + self.R\n",
    "        K = np.dot(np.dot(self.P, self.H.T), np.linalg.inv(S))\n",
    "        self.x = self.x + np.dot(K, (z - np.dot(self.H, self.x)))\n",
    "        self.P = self.P - np.dot(np.dot(K, self.H), self.P)\n",
    "\n",
    "trackers = {}\n",
    "video_path = '1.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "frame_count = 0\n",
    "frame_skip = 1  # Process every 30th frame\n",
    "\n",
    "MATCHING_THRESHOLD = 0.5\n",
    "\n",
    "def extract_features_batch(images_and_boxes):\n",
    "    features = []\n",
    "    for image, bbox in images_and_boxes:\n",
    "        feature = feature_extractor.extract_features(image, bbox)\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "def add_tracking_info(track_id, bbox):\n",
    "    x1, y1, x2, y2 = map(int, bbox)\n",
    "    track_info.append({\n",
    "        'track_id': track_id,\n",
    "        'x1': x1,\n",
    "        'y1': y1,\n",
    "        'x2': x2,\n",
    "        'y2': y2\n",
    "    })\n",
    "\n",
    "class Annotator:\n",
    "    def __init__(self, img, line_width=2):\n",
    "        self.img = img\n",
    "        self.line_width = line_width\n",
    "\n",
    "    def box_label(self, bbox, color=(0, 255, 0), label=None):\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        cv2.rectangle(self.img, (x1, y1), (x2, y2), color, thickness=self.line_width)\n",
    "        if label:\n",
    "            (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "            cv2.rectangle(self.img, (x1, y1 - h - 10), (x1 + w, y1), color, -1)\n",
    "            cv2.putText(self.img, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "\n",
    "try:\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if success:\n",
    "            frame_count += 1\n",
    "            if frame_count % frame_skip != 0:\n",
    "                continue \n",
    "\n",
    "            results = model.track(frame, persist=True, verbose=False)\n",
    "\n",
    "            if results[0].boxes.id is not None:\n",
    "                clss = results[0].boxes.cls.cpu().tolist()\n",
    "                boxes = results[0].boxes.xyxy.cpu()\n",
    "                track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "\n",
    "                annotator = Annotator(frame, line_width=2)\n",
    "\n",
    "                detections = []\n",
    "                for box, cls in zip(boxes, clss):\n",
    "                    class_name = names[int(cls)]\n",
    "                    if class_name == \"person\":\n",
    "                        detections.append((frame, box))\n",
    "\n",
    "                with ThreadPoolExecutor() as executor:\n",
    "                    features_batch = list(executor.map(extract_features_batch, [detections]))\n",
    "\n",
    "                detections = [(box, feature) for (frame, box), feature in zip(detections, features_batch[0])]\n",
    "\n",
    "                # Re-identification and tracking update\n",
    "                if len(track_features) > 0:\n",
    "                    cost_matrix = np.zeros((len(track_features), len(detections)))\n",
    "                    for i, (track_id, track_feature) in enumerate(track_features.items()):\n",
    "                        for j, (box, detection_feature) in enumerate(detections):\n",
    "                            cost_matrix[i, j] = 1 - cosine_similarity(track_feature.reshape(1, -1), detection_feature.reshape(1, -1)).item()\n",
    "\n",
    "                    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "                    assigned_tracks = {}\n",
    "                    for i, j in zip(row_ind, col_ind):\n",
    "                        if cost_matrix[i, j] < MATCHING_THRESHOLD:\n",
    "                            track_id = list(track_features.keys())[i]\n",
    "                            box, features = detections[j]\n",
    "                            trackers[track_id].update(np.array([[((box[0] + box[2]) / 2)], [(box[1] + box[3]) / 2]]))\n",
    "                            assigned_tracks[track_id] = box, features\n",
    "                            detections[j] = None\n",
    "                            add_tracking_info(track_id, box)\n",
    "\n",
    "                    detections = [d for d in detections if d is not None]\n",
    "\n",
    "                    for track_id, (box, features) in assigned_tracks.items():\n",
    "                        annotator.box_label(box, color=(255, 0, 0), label=f\"person {track_id}\")\n",
    "                        track_features[track_id] = features\n",
    "                        track = track_history[track_id]\n",
    "                        track.append((int((box[0] + box[2]) / 2), int((box[1] + box[3]) / 2)))\n",
    "                        if len(track) > 30:\n",
    "                            track.pop(0)\n",
    "                        points = np.array(track, dtype=np.int32).reshape((-1, 1, 2))\n",
    "                        cv2.circle(frame, track[-1], 7, (255, 0, 0), -1)\n",
    "                        cv2.polylines(frame, [points], isClosed=False, color=(255, 0, 0), thickness=2)\n",
    "                        cv2.putText(frame, f\"ID: {track_id}\", (int(box[2]) + 10, int(box[1]) + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "                for box, features in detections:\n",
    "                    new_track_id = next_track_id\n",
    "                    next_track_id += 1\n",
    "                    trackers[new_track_id] = KalmanFilter(dt=1, u_x=0, u_y=0, std_acc=1, x_std_meas=0.1, y_std_meas=0.1)\n",
    "                    trackers[new_track_id].update(np.array([[((box[0] + box[2]) / 2)], [(box[1] + box[3]) / 2]]))\n",
    "                    track_features[new_track_id] = features\n",
    "                    track_history[new_track_id].append((int((box[0] + box[2]) / 2), int((box[1] + box[3]) / 2)))\n",
    "                    annotator.box_label(box, color=(0, 255, 0), label=f\"person {new_track_id}\")\n",
    "                    cv2.circle(frame, track_history[new_track_id][-1], 7, (0, 255, 0), -1)\n",
    "                    cv2.putText(frame, f\"ID: {new_track_id}\", (int(box[2]) + 10, int(box[1]) + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "                    add_tracking_info(new_track_id, box)\n",
    "\n",
    "            df = pd.DataFrame(track_info)\n",
    "            df.to_csv('tracking_info.csv', index=False)\n",
    "            display(Image(data=cv2.imencode('.jpg', frame)[1].tobytes()))\n",
    "            clear_output(wait=True)\n",
    "        else:\n",
    "            break\n",
    "finally:\n",
    "    cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
